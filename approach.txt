1. Do the DB stuff to get a list of URLs to check - Tom R might have a query for this. Copy down a production DB
2. Create a simple rule in json - one of the things from the nomensa audit
3. Write code to read rules, read in urls, parse urls looking for the pattern/rule
4. Output files which match the pattern.

code breakdown
executable with command line options
Object to read in the rules and hold them in a convenient structure
Object to (yield) return the Urls for checking
Object to hold details of urls where the patterns have been found - this should also output the result

1. Production DB downloaded locally and restored
2. Had to modify robots.txt so that I could 'crawl' the site
3. URLs acquired with   wget -m http://www.example.com 2>&1 | grep '^--' | awk '{ print $3 }' | grep -v '\.\(css\|js\|png\|gif\|jpg\|JPG\)$' > urls.txt
4. Run parse_found_urls.py on the urls.txt file to 'tidy up' the urls
